# ==============================================
# Model Configuration
# ==============================================
base_model: meta-llama/Llama-3.2-1B-Instruct
tokenizer_type: meta-llama/Llama-3.2-1B-Instruct

# ==============================================
# Dataset
# ==============================================
dataset:
  name: knkarthick/samsum
  cache_dir: ../data/datasets
  field_map:
    input: dialogue
    output: summary
  type: completion
  splits:
    train: 100
    validation: 200
    test: 200
  seed: 42

task_instruction: >
  You are a helpful assistant who writes concise, factual summaries of conversations.
  Summarize the following conversation into a single sentence.

# ==============================================
# NO LoRA Configuration = Full Fine-tuning
# ==============================================
# (Remove or comment out lora_r to disable LoRA)

# ==============================================
# Training Configuration
# ==============================================
num_epochs: 1
learning_rate: 5e-5 # Lower LR for full fine-tuning
batch_size: 2
eval_batch_size: 2
gradient_accumulation_steps: 8
sequence_len: 512
lr_scheduler: cosine
warmup_steps: 100
bf16: true
logging_steps: 25
optim: adamw_torch

# ==============================================
# Output & Logging
# ==============================================
wandb_project: llama3_samsum
wandb_run_name: fsdp-full-finetuning
