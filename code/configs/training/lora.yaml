# ==============================================
# Model Configuration
# ==============================================
base_model: meta-llama/Llama-3.2-1B-Instruct
tokenizer_type: meta-llama/Llama-3.2-1B-Instruct

# ==============================================
# Dataset
# ==============================================
dataset:
  name: knkarthick/samsum
  cache_dir: ../data/datasets
  field_map:
    input: dialogue
    output: summary
  type: completion
  splits:
    train: 100
    validation: 200
    test: 200
  seed: 42

task_instruction: >
  You are a helpful assistant who writes concise, factual summaries of conversations.
  Summarize the following conversation into a single sentence.

# ==============================================
# LoRA Configuration (present = LoRA enabled)
# ==============================================
lora_r: 16
lora_alpha: 32
lora_dropout: 0.1
target_modules: ["q_proj", "v_proj", "k_proj", "o_proj"]

# ==============================================
# Training Configuration
# ==============================================
num_epochs: 1
learning_rate: 2e-4
batch_size: 4
eval_batch_size: 4
gradient_accumulation_steps: 4
sequence_len: 512
lr_scheduler: cosine
warmup_steps: 50
bf16: true
logging_steps: 25
optim: adamw_torch # Use standard AdamW (no quantization)

# ==============================================
# Output & Logging
# ==============================================
wandb_project: llama3_samsum
wandb_run_name: fsdp-lora-finetuning
