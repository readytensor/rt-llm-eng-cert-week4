{
  "rouge1": 0.5312356628825021,
  "rouge2": 0.30733815618079036,
  "rougeL": 0.45697083987301657,
  "num_samples": 200,
  "base_model": "meta-llama/Llama-3.1-8B-Instruct",
  "adapter_dir": "/workspace/rt-llm-eng-cert-week4/data/outputs/lora_samsum/llama-8b-1-gpu/lora_adapters"
}